{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e092e45",
   "metadata": {},
   "source": [
    "## Data Mining and Visualization\n",
    "\n",
    "### *Project 2 - Activity Detection using Embedded Machine Learning*\n",
    "\n",
    "### By Shaheer Khan\n",
    "\n",
    "------------------------------------------------------------------------------------------\n",
    "\n",
    "The task is to build a Machine learning Model that can accurately turn dozens of raw “wiggles-in-time” (accelerometer + gyroscope CSV files) into a small program that can look at those wiggles and say “this person is walking / running / sitting /”\n",
    "\n",
    "\n",
    "This dataset has been collected to be used in Edge Impulse documentation. This dataset has been used to create an Embedded Machine Learning - Activity Detection project using different steps.\n",
    "\n",
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714812fe",
   "metadata": {},
   "source": [
    "\n",
    "### 1 - *Load and Peek* \n",
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3b29f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from math import sqrt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import scipy.stats as st\n",
    "import itertools  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8499ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recording</th>\n",
       "      <th>acc_files</th>\n",
       "      <th>gyro_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cycling-2023-09-14_06-22-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cycling-2023-09-14_06-33-47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cycling-2023-09-14_06-47-00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cycling-2023-09-16_07-43-07</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cycling-2023-09-16_09-25-09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     recording  acc_files  gyro_files\n",
       "0  Cycling-2023-09-14_06-22-31          1           1\n",
       "1  Cycling-2023-09-14_06-33-47          1           1\n",
       "2  Cycling-2023-09-14_06-47-00          1           1\n",
       "3  Cycling-2023-09-16_07-43-07          1           1\n",
       "4  Cycling-2023-09-16_09-25-09          1           1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "                           recording  acc_files  gyro_files\n",
      "count                            12       12.0        12.0\n",
      "unique                           12        NaN         NaN\n",
      "top     Cycling-2023-09-14_06-22-31        NaN         NaN\n",
      "freq                              1        NaN         NaN\n",
      "mean                            NaN        1.0         1.0\n",
      "std                             NaN        0.0         0.0\n",
      "min                             NaN        1.0         1.0\n",
      "25%                             NaN        1.0         1.0\n",
      "50%                             NaN        1.0         1.0\n",
      "75%                             NaN        1.0         1.0\n",
      "max                             NaN        1.0         1.0\n",
      "\n",
      "All recordings have exactly one acc + one gyro file.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"/Users/shaheer/documents/semester 5/dmv/project 2/archive\")     \n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Folder {DATA_DIR} not found. \"\n",
    "                            \"Point DATA_DIR to your unzipped archive.\")\n",
    "\n",
    "def inspect_dataset(root: Path):\n",
    "    rows = []\n",
    "    for rec_folder in sorted(root.iterdir()):\n",
    "        if not rec_folder.is_dir():\n",
    "            continue\n",
    "        acc = list(rec_folder.glob(\"accelerometer.csv\"))\n",
    "        gyr = list(rec_folder.glob(\"gyroscope.csv\"))\n",
    "        rows.append({\n",
    "            \"recording\": rec_folder.name,\n",
    "            \"acc_files\": len(acc),\n",
    "            \"gyro_files\": len(gyr)\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_overview = inspect_dataset(DATA_DIR)\n",
    "display(df_overview.head())      # shows first few rows\n",
    "print(\"\\nSummary:\\n\", df_overview.describe(include='all'))\n",
    "missing = df_overview[(df_overview.acc_files != 1) | (df_overview.gyro_files != 1)]\n",
    "if not missing.empty:\n",
    "    print(\"\\nWARNING: some recordings are incomplete:\\n\", missing)\n",
    "else:\n",
    "    print(\"\\nAll recordings have exactly one acc + one gyro file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d00e0",
   "metadata": {},
   "source": [
    "###\n",
    "### 2 - *Read & Fuse each pair* \n",
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e2bcb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycling-2023-09-14_06-22-31: 71848 samples: Cycling-2023-09-14_06-22-31_fused.csv\n",
      "Cycling-2023-09-14_06-33-47: 170718 samples: Cycling-2023-09-14_06-33-47_fused.csv\n",
      "Cycling-2023-09-14_06-47-00: 144458 samples: Cycling-2023-09-14_06-47-00_fused.csv\n",
      "Cycling-2023-09-16_07-43-07: 408516 samples: Cycling-2023-09-16_07-43-07_fused.csv\n",
      "Cycling-2023-09-16_09-25-09: 520472 samples: Cycling-2023-09-16_09-25-09_fused.csv\n",
      "Cycling-2023-10-18_06-36-17: 212000 samples: Cycling-2023-10-18_06-36-17_fused.csv\n",
      "Cycling-2023-10-18_06-51-26: 135721 samples: Cycling-2023-10-18_06-51-26_fused.csv\n",
      "Sitting-2023-09-14_08-37-45: 185402 samples: Sitting-2023-09-14_08-37-45_fused.csv\n",
      "Sitting-2023-09-14_09-11-15: 209622 samples: Sitting-2023-09-14_09-11-15_fused.csv\n",
      "Sitting-2023-10-18_09-05-37: 117012 samples: Sitting-2023-10-18_09-05-37_fused.csv\n",
      "Walking-2023-09-14_21-51-59: 115810 samples: Walking-2023-09-14_21-51-59_fused.csv\n",
      "Walking-2023-09-16_18-14-40: 484348 samples: Walking-2023-09-16_18-14-40_fused.csv\n",
      "\n",
      "All done — fused files live in ./fused/\n"
     ]
    }
   ],
   "source": [
    "# STEP 1 - Fuse accelerometer + gyroscope CSVs → one 6-axis file per recording\n",
    " \n",
    "RAW_DIR   = Path(\"/Users/shaheer/documents/semester 5/dmv/project 2/archive\")         \n",
    "FUSED_DIR = Path(\"/Users/shaheer/documents/semester 5/dmv/project 2/fused\")\n",
    "FUSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def load_and_standardise(path, sensor_type):\n",
    "    \"\"\"\n",
    "    Reads a CSV and returns a dataframe with:\n",
    "    ['timestamp', 'ax', 'ay', 'az']  or  ['timestamp', 'gx', 'gy', 'gz']\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Detect likely timestamp column\n",
    "    ts_col = [c for c in df.columns if 'time' in c.lower()][0]\n",
    "    df = df.rename(columns={ts_col: 'timestamp'})\n",
    "    \n",
    "    # Rename axis columns to ax/ay/az or gx/gy/gz\n",
    "    axis_cols = [c for c in df.columns if c != 'timestamp'][:3]\n",
    "    new_names = [f\"{sensor_type[0]}{axis}\" for axis in ('x', 'y', 'z')]\n",
    "    df = df.rename(columns=dict(zip(axis_cols, new_names)))\n",
    "    \n",
    "    # Keep only needed columns\n",
    "    return df[['timestamp'] + new_names]\n",
    "\n",
    "for folder in sorted(RAW_DIR.iterdir()):\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "    \n",
    "    acc_file  = next(folder.glob(\"accelerometer.csv\"),  None)\n",
    "    gyro_file = next(folder.glob(\"gyroscope.csv\"), None)\n",
    "    if not acc_file or not gyro_file:\n",
    "        print(f\"Skipping {folder.name}: missing sensor file.\")\n",
    "        continue\n",
    "    \n",
    "    # 1. Load & clean\n",
    "    df_acc  = load_and_standardise(acc_file,  'acc')\n",
    "    df_gyro = load_and_standardise(gyro_file, 'gyro')\n",
    "    \n",
    "    # 2. Normalise timestamps (start at 0 ms)\n",
    "    t0 = min(df_acc.timestamp.min(), df_gyro.timestamp.min())\n",
    "    df_acc['timestamp']  -= t0\n",
    "    df_gyro['timestamp'] -= t0\n",
    "    \n",
    "    # 3. Merge on timestamp (outer join)\n",
    "    fused = pd.merge(df_acc, df_gyro, on='timestamp', how='outer').sort_values('timestamp')\n",
    "    \n",
    "    # 4. Save\n",
    "    out_path = FUSED_DIR / f\"{folder.name}_fused.csv\"\n",
    "    fused.to_csv(out_path, index=False)\n",
    "    \n",
    "    # 5. Quick log\n",
    "    print(f\"{folder.name}: {len(fused):5d} samples: {out_path.name}\")\n",
    "\n",
    "print(\"\\nAll done — fused files live in ./fused/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea3cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycling-2023-09-14_06-22-31_fused        →     0 rows  (gaps filled: 0)\n",
      "Cycling-2023-09-14_06-33-47_fused        → 19903 rows  (gaps filled: 0)\n",
      "Cycling-2023-09-14_06-47-00_fused        →  6562 rows  (gaps filled: 0)\n",
      "Cycling-2023-09-16_07-43-07_fused        → 41026 rows  (gaps filled: 0)\n",
      "Cycling-2023-09-16_09-25-09_fused        → 52079 rows  (gaps filled: 0)\n",
      "Cycling-2023-10-18_06-36-17_fused        → 14310 rows  (gaps filled: 0)\n",
      "Cycling-2023-10-18_06-51-26_fused        → 12086 rows  (gaps filled: 0)\n",
      "Sitting-2023-09-14_08-37-45_fused        → 21066 rows  (gaps filled: 0)\n",
      "Sitting-2023-09-14_09-11-15_fused        →  9308 rows  (gaps filled: 0)\n",
      "Sitting-2023-10-18_09-05-37_fused        →     0 rows  (gaps filled: 0)\n",
      "Walking-2023-09-14_21-51-59_fused        →  9220 rows  (gaps filled: 0)\n",
      "Walking-2023-09-16_18-14-40_fused        → 58618 rows  (gaps filled: 0)\n",
      "\n",
      "All done — uniformly-sampled files are in /Users/shaheer/documents/semester 5/dmv/project 2/resampled\n"
     ]
    }
   ],
   "source": [
    "# STEP 2 - Resample fused CSVs at 50 Hz (every 20 ms) and interpolate gaps\n",
    "\n",
    "FUSED_DIR      = Path(\"/Users/shaheer/documents/semester 5/dmv/project 2/fused\")\n",
    "RESAMPLED_DIR  = Path(\"/Users/shaheer/documents/semester 5/dmv/project 2/resampled\")\n",
    "RESAMPLED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TARGET_HZ      = 50                         # 50 hz\n",
    "PERIOD_MS      = 1000 / TARGET_HZ           # 20 ms\n",
    "\n",
    "# Helper: detect unit & convert timestamp column to milliseconds\n",
    "def convert_to_ms(ts_series):\n",
    "    \"\"\"\n",
    "    Return a NumPy array of timestamps in milliseconds starting at 0.\n",
    "    Auto-detects whether the raw unit is ns, µs, ms, or s.\n",
    "    \"\"\"\n",
    "    ts = ts_series.to_numpy().astype(\"float64\")\n",
    "    ts -= ts[0]                       # normalise start = 0\n",
    "    median_step = np.median(np.diff(ts))\n",
    "\n",
    "    if median_step > 1_000_000:       # nanoseconds → divide by 1 000 000\n",
    "        ts = ts / 1_000_000.0\n",
    "    elif median_step > 1_000:         # microseconds → divide by 1 000\n",
    "        ts = ts / 1_000.0\n",
    "    elif median_step < 1:             # seconds → multiply by 1 000\n",
    "        ts = ts * 1_000.0\n",
    "    # else already in milliseconds\n",
    "    return ts\n",
    "\n",
    "# Resample a single fused file \n",
    "def resample_file(path: Path):\n",
    "    df  = pd.read_csv(path)\n",
    "    df[\"timestamp\"] = convert_to_ms(df[\"timestamp\"])\n",
    "\n",
    "    # Uniform 20 ms grid\n",
    "    new_ts = np.arange(\n",
    "        0, df[\"timestamp\"].iloc[-1] + PERIOD_MS, PERIOD_MS, dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # Re-index & interpolate\n",
    "    df = (\n",
    "        df.set_index(\"timestamp\")\n",
    "          .reindex(new_ts)\n",
    "          .interpolate(\"linear\")\n",
    "          .reset_index()\n",
    "          .rename(columns={\"index\": \"timestamp\"})\n",
    "    )\n",
    "    # Trim any rows that still contain NaNs (lead/trail gaps)\n",
    "    df = df.dropna(how=\"any\")\n",
    "\n",
    "    # Save\n",
    "    out_path = RESAMPLED_DIR / path.name.replace(\"_fused\", \"_resampled\")\n",
    "    df.to_csv(out_path, index=False)\n",
    "\n",
    "    # Log line\n",
    "    print(\n",
    "        f\"{path.stem:40s} → {len(df):5d} rows  \"\n",
    "        f\"(gaps filled: {df.isna().any(axis=1).sum()})\"\n",
    "    )\n",
    "\n",
    "# Loop over all fused files \n",
    "for fused_csv in sorted(FUSED_DIR.glob(\"*_fused.csv\")):\n",
    "    resample_file(fused_csv)\n",
    "\n",
    "print(\"\\nAll done — uniformly-sampled files are in\", RESAMPLED_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55039495",
   "metadata": {},
   "source": [
    "###\n",
    "### 3 - *Window & Feature* \n",
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a057ef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 done → shapes: X (4870, 30) , y (4870,) , groups (4870,)\n"
     ]
    }
   ],
   "source": [
    "# STEP 3 – windowing (2 s, 50 % overlap) + classic statistical features\n",
    "\n",
    "# parameters \n",
    "RESAMPLED_DIR   = Path(\"/Users/shaheer/documents/semester 5/dmv/project 2/resampled\")\n",
    "WINDOW_SEC      = 2               # 2-second windows\n",
    "TARGET_HZ       = 50              # resampled to 50 Hz\n",
    "WIN_SAMPLES     = WINDOW_SEC * TARGET_HZ      # 100\n",
    "STRIDE_SAMPLES  = WIN_SAMPLES // 2            # 50  (50 % overlap)\n",
    "AXES            = [\"ax\", \"ay\", \"az\", \"gx\", \"gy\", \"gz\"]\n",
    "FEATURE_FUNCS   = {\n",
    "    \"mean\": np.mean,\n",
    "    \"std\":  np.std,\n",
    "    \"min\":  np.min,\n",
    "    \"max\":  np.max,\n",
    "    \"rms\":  lambda x: sqrt(np.mean(np.square(x)))\n",
    "}\n",
    "\n",
    "# helper: compute 30-feature vector from a window (shape 100×6) \n",
    "def extract_features(window_ndarray):\n",
    "    feats = []\n",
    "    for i, axis in enumerate(AXES):          # axis order ax…gz\n",
    "        col = window_ndarray[:, i]\n",
    "        for name, fn in FEATURE_FUNCS.items():\n",
    "            feats.append(fn(col))\n",
    "    return feats                             # length = 6 axes × 5 funcs = 30\n",
    "\n",
    "# main loop \n",
    "X, y, groups = [], [], []                # add groups list here\n",
    "\n",
    "for csv_path in sorted(RESAMPLED_DIR.glob(\"*_resampled.csv\")):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    activity = csv_path.stem.split(\"-\")[0]\n",
    "\n",
    "    data = df[AXES].to_numpy(dtype=np.float32)\n",
    "    for start in range(0, len(data) - WIN_SAMPLES + 1, STRIDE_SAMPLES):\n",
    "        window = data[start : start + WIN_SAMPLES]\n",
    "        feats  = extract_features(window)\n",
    "        X.append(feats)\n",
    "        y.append(activity)\n",
    "        groups.append(csv_path.stem)     # tag each window\n",
    "\n",
    "# convert & save \n",
    "X       = np.asarray(X, dtype=np.float32)\n",
    "y       = np.asarray(y)\n",
    "groups  = np.asarray(groups)            # to numpy\n",
    "\n",
    "np.save(\"X.npy\", X)\n",
    "np.save(\"y.npy\", y)\n",
    "np.save(\"groups.npy\", groups)           # save groups\n",
    "\n",
    "meta = {\"feature_order\": [f\"{axis}_{stat}\"\n",
    "                          for axis in AXES\n",
    "                          for stat in FEATURE_FUNCS]}\n",
    "with open(\"features_meta.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"Step 3 done → shapes:\",\n",
    "      \"X\", X.shape, \", y\", y.shape, \", groups\", groups.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54344ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes →  X: (4870, 30)  |  y: (4870,)  |  groups: (4870,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity</th>\n",
       "      <th>Windows</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cycling</td>\n",
       "      <td>2911</td>\n",
       "      <td>59.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Walking</td>\n",
       "      <td>1354</td>\n",
       "      <td>27.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sitting</td>\n",
       "      <td>605</td>\n",
       "      <td>12.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activity  Windows  Percent\n",
       "0  Cycling     2911    59.77\n",
       "1  Walking     1354    27.80\n",
       "2  Sitting      605    12.42"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total windows: 4870\n"
     ]
    }
   ],
   "source": [
    "# STEP 4 – Checking Class Balance\n",
    "\n",
    "# load \n",
    "DATA_DIR = Path(\"/Users/shaheer/documents/semester 5/dmv/project 2\")\n",
    "X       = np.load(DATA_DIR / \"X.npy\")\n",
    "y       = np.load(DATA_DIR / \"y.npy\")\n",
    "groups  = np.load(DATA_DIR / \"groups.npy\")   \n",
    "\n",
    "print(\"Shapes →  X:\", X.shape, \" |  y:\", y.shape, \" |  groups:\", groups.shape)\n",
    "\n",
    "# class-balance summary \n",
    "counts = Counter(y)\n",
    "total  = len(y)\n",
    "balance_df = (pd.DataFrame({\"Activity\": counts.keys(),\n",
    "                            \"Windows\": counts.values(),\n",
    "                            \"Percent\": [round(100*c/total,2) for c in counts.values()]})\n",
    "                .sort_values(\"Windows\", ascending=False)\n",
    "                .reset_index(drop=True))\n",
    "display(balance_df)\n",
    "print(\"\\nTotal windows:\", total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be568a22",
   "metadata": {},
   "source": [
    "###\n",
    "### 4 - *Quick baseline with a Random Forest* \n",
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0b1ad41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped 5-fold cross-validation (each fold = unseen recordings)\n",
      "\n",
      "Fold 1  accuracy = 0.000\n",
      "          per-class F1: {'Cycling': 0.0, 'Walking': 0.0}\n",
      "Fold 2  accuracy = 0.226\n",
      "          per-class F1: {'Cycling': 0.369, 'Walking': 0.0}\n",
      "Fold 3  accuracy = 0.700\n",
      "          per-class F1: {'Cycling': 0.823, 'Walking': 0.0}\n",
      "Fold 4  accuracy = 0.188\n",
      "          per-class F1: {'Cycling': 0.317, 'Sitting': 0.0, 'Walking': 0.0}\n",
      "Fold 5  accuracy = 0.788\n",
      "          per-class F1: {'Cycling': 0.882, 'Walking': 0.0}\n",
      "\n",
      "Mean grouped-fold accuracy: 0.38\n",
      "\n",
      "Saved final model → /Users/shaheer/documents/semester 5/dmv/project 2/activity_rf.pkl\n"
     ]
    }
   ],
   "source": [
    "# STEP 5 – Baseline classifier: Random Forest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib, pprint, textwrap\n",
    "import warnings, pprint\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")   # keep output clean\n",
    "\n",
    "# load feature matrix & labels\n",
    "DATA_DIR = Path(\"/Users/shaheer/documents/semester 5/dmv/project 2\")\n",
    "X       = np.load(DATA_DIR / \"X.npy\")\n",
    "y       = np.load(DATA_DIR / \"y.npy\")\n",
    "groups  = np.load(DATA_DIR / \"groups.npy\")\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "fold_scores = []\n",
    "\n",
    "print(\"Grouped 5-fold cross-validation (each fold = unseen recordings)\\n\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(gkf.split(X, y, groups), 1):\n",
    "    rf = RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=fold)          # different seed per fold for variety\n",
    "\n",
    "    rf.fit(X[train_idx], y[train_idx])\n",
    "    y_pred = rf.predict(X[test_idx])\n",
    "\n",
    "    rpt = classification_report(y[test_idx], y_pred, digits=3, output_dict=True)\n",
    "    fold_scores.append(rpt[\"accuracy\"])\n",
    "\n",
    "    print(f\"Fold {fold}  accuracy = {rpt['accuracy']:.3f}\")\n",
    "    # Pretty-print per-class F1\n",
    "    per_class = {lbl: round(met['f1-score'], 3)\n",
    "                 for lbl, met in rpt.items() if lbl in np.unique(y)}\n",
    "    print(\"          per-class F1:\", per_class)\n",
    "\n",
    "print(\"\\nMean grouped-fold accuracy:\", round(np.mean(fold_scores), 3))\n",
    "\n",
    "# train on all data and save the model\n",
    "final_rf = RandomForestClassifier(\n",
    "              n_estimators=400,\n",
    "              class_weight=\"balanced\",\n",
    "              random_state=42)\n",
    "final_rf.fit(X, y)\n",
    "joblib.dump(final_rf, DATA_DIR / \"activity_rf.pkl\")\n",
    "print(\"\\nSaved final model →\", (DATA_DIR / 'activity_rf.pkl').resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149bf1e8",
   "metadata": {},
   "source": [
    "###\n",
    "### Improving Results \n",
    "### Path A\n",
    "### 3 (Redo) - *Build richer features & Evaluate* \n",
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e0abf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched feature matrix shape: (4870, 72)\n"
     ]
    }
   ],
   "source": [
    "RESAMPLED_DIR  = Path(\"/Users/shaheer/documents/semester 5/dmv/project 2/resampled\")\n",
    "WINDOW_SEC, HZ = 2, 50\n",
    "WIN_SAMPLES    = WINDOW_SEC * HZ\n",
    "STRIDE         = WIN_SAMPLES // 2\n",
    "\n",
    "# Original 6 axes + magnitudes\n",
    "AXES = [\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\"]\n",
    "\n",
    "#  helper functions \n",
    "def band_energy(sig, lo, hi):\n",
    "    fft   = np.fft.rfft(sig * np.hanning(len(sig)))\n",
    "    freqs = np.fft.rfftfreq(len(sig), d=1/HZ)\n",
    "    band  = (freqs >= lo) & (freqs < hi)\n",
    "    return float(np.sum(np.abs(fft[band])**2) / band.sum())\n",
    "\n",
    "def feats_1d(x):\n",
    "    return [\n",
    "        np.mean(x), np.std(x), np.median(x),\n",
    "        np.percentile(x,75)-np.percentile(x,25),   # IQR\n",
    "        np.ptp(x),                                 # range\n",
    "        st.skew(x), st.kurtosis(x,fisher=False),\n",
    "        band_energy(x,0,3), band_energy(x,3,6)     # two cadence bands\n",
    "    ]\n",
    "\n",
    "# build windows \n",
    "X_enriched, y, groups = [], [], []\n",
    "\n",
    "for csv in sorted(RESAMPLED_DIR.glob(\"*_resampled.csv\")):\n",
    "    df = pd.read_csv(csv)\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    # add magnitudes\n",
    "    df[\"acc_mag\"]  = np.sqrt((df[[\"ax\",\"ay\",\"az\"]]**2).sum(axis=1))\n",
    "    df[\"gyro_mag\"] = np.sqrt((df[[\"gx\",\"gy\",\"gz\"]]**2).sum(axis=1))\n",
    "    sig_cols = AXES + [\"acc_mag\",\"gyro_mag\"]          # 8 signals\n",
    "    data     = df[sig_cols].to_numpy(np.float32)\n",
    "\n",
    "    label = csv.stem.split(\"-\")[0]\n",
    "\n",
    "    for start in range(0, len(data)-WIN_SAMPLES+1, STRIDE):\n",
    "        win = data[start:start+WIN_SAMPLES]\n",
    "        row = []\n",
    "        for i in range(win.shape[1]):                 # 8 signals\n",
    "            row.extend(feats_1d(win[:, i]))\n",
    "        X_enriched.append(row)\n",
    "        y.append(label)\n",
    "        groups.append(csv.stem)\n",
    "\n",
    "X_enriched = np.asarray(X_enriched, np.float32)\n",
    "y          = np.asarray(y)\n",
    "groups     = np.asarray(groups)\n",
    "\n",
    "print(\"Enriched feature matrix shape:\", X_enriched.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c0922be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save without overwriting originals \n",
    "\n",
    "np.save(\"X_enriched.npy\", X_enriched)\n",
    "np.save(\"y.npy\",            y)            \n",
    "np.save(\"groups_enriched.npy\", groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b765b1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1  accuracy 0.000\n",
      "Fold 2  accuracy 0.468\n",
      "Fold 3  accuracy 0.720\n",
      "Fold 4  accuracy 0.251\n",
      "Fold 5  accuracy 0.788\n",
      "\n",
      "Mean grouped-recording accuracy: 0.445\n",
      "Saved: activity_rf_enriched.pkl\n"
     ]
    }
   ],
   "source": [
    "# grouped 5-fold RF evaluation \n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "fold_acc = []\n",
    "for fold,(tr,te) in enumerate(gkf.split(X_enriched, y, groups),1):\n",
    "    rf = RandomForestClassifier(n_estimators=400,\n",
    "                                class_weight=\"balanced\",\n",
    "                                random_state=fold)\n",
    "    rf.fit(X_enriched[tr], y[tr])\n",
    "    y_pred = rf.predict(X_enriched[te])\n",
    "    rpt = classification_report(y[te], y_pred, digits=3, output_dict=True)\n",
    "    fold_acc.append(rpt[\"accuracy\"])\n",
    "    print(f\"Fold {fold}  accuracy {rpt['accuracy']:.3f}\")\n",
    "\n",
    "print(\"\\nMean grouped-recording accuracy:\",\n",
    "      round(float(np.mean(fold_acc)),3))\n",
    "\n",
    "# save final model \n",
    "rf_final = RandomForestClassifier(n_estimators=400,\n",
    "                                  class_weight=\"balanced\",\n",
    "                                  random_state=42)\n",
    "rf_final.fit(X_enriched, y)\n",
    "import joblib\n",
    "joblib.dump(rf_final, \"activity_rf_enriched.pkl\")\n",
    "print(\"Saved: activity_rf_enriched.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a63d7a",
   "metadata": {},
   "source": [
    "###\n",
    "### Improving Results \n",
    "### Path B\n",
    "### 3 (Redo) - *Raw-window pipeline + 1-D CNN* \n",
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc4bd6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw window tensor: (4870, 100, 6)\n",
      "Classes: ['Cycling', 'Sitting', 'Walking']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import layers, models\n",
    "import os, time\n",
    "\n",
    "# parameters \n",
    "RESAMPLED_DIR  = Path(\"resampled\")\n",
    "WIN_SEC, HZ    = 2, 50\n",
    "WIN_SAMPLES    = WIN_SEC * HZ         # 100\n",
    "STRIDE         = WIN_SAMPLES // 2     # 50 (50 % overlap)\n",
    "AXES           = [\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\"]\n",
    "\n",
    "# 1. build raw-window dataset \n",
    "X_raw, y, groups = [], [], []\n",
    "\n",
    "for csv in sorted(RESAMPLED_DIR.glob(\"*_resampled.csv\")):\n",
    "    df = pd.read_csv(csv)\n",
    "    if df.empty:                   # skip trimmed-out recordings\n",
    "        continue\n",
    "\n",
    "    data   = df[AXES].to_numpy(np.float32)\n",
    "    label  = csv.stem.split(\"-\")[0]\n",
    "\n",
    "    for start in range(0, len(data) - WIN_SAMPLES+1, STRIDE):\n",
    "        X_raw.append(data[start:start + WIN_SAMPLES])\n",
    "        y.append(label)\n",
    "        groups.append(csv.stem)\n",
    "\n",
    "X_raw   = np.asarray(X_raw, dtype=np.float32)     # (N,100,6)\n",
    "y       = np.asarray(y)\n",
    "groups  = np.asarray(groups)\n",
    "\n",
    "np.save(\"X_raw.npy\",   X_raw)\n",
    "np.save(\"y.npy\",       y)            # same labels reused\n",
    "np.save(\"groups_raw.npy\", groups)\n",
    "\n",
    "print(\"Raw window tensor:\", X_raw.shape)\n",
    "\n",
    "# 2. encode labels → integers \n",
    "le = LabelEncoder(); y_int = le.fit_transform(y)\n",
    "n_classes = len(le.classes_)\n",
    "print(\"Classes:\", list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3d847c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "— Fold 1: training on 4687 windows —\n",
      "   accuracy 0.000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Cycling      0.000     0.000     0.000       0.0\n",
      "     Sitting      0.000     0.000     0.000       0.0\n",
      "     Walking      0.000     0.000     0.000     183.0\n",
      "\n",
      "   micro avg      0.000     0.000     0.000     183.0\n",
      "   macro avg      0.000     0.000     0.000     183.0\n",
      "weighted avg      0.000     0.000     0.000     183.0\n",
      "\n",
      "\n",
      "— Fold 2: training on 4630 windows —\n",
      "   accuracy 1.000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Cycling      1.000     1.000     1.000       240\n",
      "     Sitting      0.000     0.000     0.000         0\n",
      "     Walking      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      1.000     1.000     1.000       240\n",
      "   macro avg      0.333     0.333     0.333       240\n",
      "weighted avg      1.000     1.000     1.000       240\n",
      "\n",
      "\n",
      "— Fold 3: training on 3514 windows —\n",
      "   accuracy 0.000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Cycling      0.000     0.000     0.000       0.0\n",
      "     Sitting      0.000     0.000     0.000     185.0\n",
      "     Walking      0.000     0.000     0.000    1171.0\n",
      "\n",
      "    accuracy                          0.000    1356.0\n",
      "   macro avg      0.000     0.000     0.000    1356.0\n",
      "weighted avg      0.000     0.000     0.000    1356.0\n",
      "\n",
      "\n",
      "— Fold 4: training on 3501 windows —\n",
      "   accuracy 0.000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Cycling      0.000     0.000     0.000     949.0\n",
      "     Sitting      0.000     0.000     0.000     420.0\n",
      "     Walking      0.000     0.000     0.000       0.0\n",
      "\n",
      "    accuracy                          0.000    1369.0\n",
      "   macro avg      0.000     0.000     0.000    1369.0\n",
      "weighted avg      0.000     0.000     0.000    1369.0\n",
      "\n",
      "\n",
      "— Fold 5: training on 3148 windows —\n",
      "   accuracy 0.200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Cycling      1.000     0.200     0.333      1722\n",
      "     Sitting      0.000     0.000     0.000         0\n",
      "     Walking      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.200      1722\n",
      "   macro avg      0.333     0.067     0.111      1722\n",
      "weighted avg      1.000     0.200     0.333      1722\n",
      "\n",
      "\n",
      "Mean grouped-fold accuracy: 0.24\n",
      "\n",
      "Saved final CNN → activity_cnn.h5  and label_map.json\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# 3. tiny CNN builder fn \n",
    "def build_cnn():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(WIN_SAMPLES, len(AXES))),\n",
    "        layers.Conv1D(32, 5, activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Conv1D(64, 5, activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    # use legacy.Adam \n",
    "    model.compile(\n",
    "        optimizer=optimizers.legacy.Adam(1e-3),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 4. 5-fold stratified group CV  (robust)  \n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (tr, te) in enumerate(sgkf.split(X_raw, y_int, groups), 1):\n",
    "    cnn = build_cnn()\n",
    "    print(f\"\\n— Fold {fold}: training on {len(tr)} windows —\")\n",
    "    cnn.fit(X_raw[tr], y_int[tr], epochs=12, batch_size=64, verbose=0)\n",
    "    \n",
    "    y_pred = cnn.predict(X_raw[te], verbose=0).argmax(1)\n",
    "    acc    = (y_pred == y_int[te]).mean()            # ← manual accuracy\n",
    "    fold_scores.append(acc)\n",
    "    \n",
    "    rpt = classification_report(\n",
    "            y_int[te], y_pred,\n",
    "            labels=np.arange(n_classes),             \n",
    "            target_names=le.classes_,\n",
    "            zero_division=0, digits=3)\n",
    "    \n",
    "    print(f\"   accuracy {acc:.3f}\\n{rpt}\")\n",
    "\n",
    "print(\"\\nMean grouped-fold accuracy:\", round(float(np.mean(fold_scores)), 3))\n",
    "\n",
    "\n",
    "# 5. train on full data & save model \n",
    "final_cnn = build_cnn()\n",
    "final_cnn.fit(X_raw, y_int, epochs=12, batch_size=64, verbose=0)\n",
    "final_cnn.save(\"activity_cnn.h5\")\n",
    "with open(\"label_map.json\",\"w\") as f: json.dump(le.classes_.tolist(), f)\n",
    "print(\"\\nSaved final CNN → activity_cnn.h5  and label_map.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc543e0",
   "metadata": {},
   "source": [
    "###\n",
    "### Improving Results \n",
    "### Path C \n",
    "### Cell A - *Build 114-feature dataset*\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7283cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 3 – spectral DSP features (114 dims)  … run once\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, scipy.signal as sg, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a4676be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X186.npy (4870, 186) |  labels (array(['Cycling', 'Sitting', 'Walking'], dtype='<U7'), array([2911,  605, 1354]))\n",
      "Total pseudo-recordings: 168\n"
     ]
    }
   ],
   "source": [
    "RESAMPLED_DIR = Path(\"/Users/shaheer/documents/semester 5/dmv/project 2/resampled\")\n",
    "HZ_IN, HZ_OUT = 50, 5\n",
    "DECIM         = HZ_IN // HZ_OUT          # 10\n",
    "FFT_LEN       = 64                       # as in EI GUI\n",
    "B, A = sg.butter(6, 2.68/(HZ_IN/2), btype='low')\n",
    "\n",
    "WIN_SEC, STRIDE_SEC = 2, 1               # 50 % overlap\n",
    "WIN_SAMPLES  = WIN_SEC   * HZ_IN         # 100\n",
    "STRIDE_SAMP  = STRIDE_SEC * HZ_IN        # 50\n",
    "AXES         = [\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\"]\n",
    "\n",
    "def spectral_feat(window_50hz):\n",
    "    feats = []\n",
    "    for i in range(6):\n",
    "        sig = window_50hz[:, i].copy()\n",
    "        if i < 3:                       # scale accel axes\n",
    "            sig *= 0.04\n",
    "        sig_f = sg.lfilter(B, A, sg.decimate(sig, DECIM, ftype='fir', zero_phase=True))\n",
    "        spec = np.abs(np.fft.rfft(sig_f, n=FFT_LEN))[:31]   # keep 31 bins\n",
    "        feats.extend(np.log(spec + 1e-6))\n",
    "    return feats\n",
    "\n",
    "CHUNK_SEC   = 30\n",
    "CHUNK_SAMP  = CHUNK_SEC * HZ_IN\n",
    "\n",
    "X186, y, groups = [], [], []\n",
    "\n",
    "for csv in sorted(RESAMPLED_DIR.glob(\"*_resampled.csv\")):\n",
    "    df = pd.read_csv(csv)\n",
    "    if df.empty: continue\n",
    "    label = csv.stem.split(\"-\")[0]\n",
    "    data  = df[AXES].to_numpy(np.float32)\n",
    "\n",
    "    for start in range(0, len(data)-WIN_SAMPLES+1, STRIDE_SAMP):\n",
    "        win = data[start:start+WIN_SAMPLES]\n",
    "        X186.append(spectral_feat(win))\n",
    "        y.append(label)\n",
    "        chunk_id = f\"{csv.stem}_chunk{start//CHUNK_SAMP}\"\n",
    "        groups.append(chunk_id)\n",
    "\n",
    "X186   = np.asarray(X186, np.float32)\n",
    "y      = np.asarray(y)\n",
    "groups = np.asarray(groups)\n",
    "\n",
    "np.save(\"X186.npy\", X186); np.save(\"y.npy\", y); np.save(\"groups186.npy\", groups)\n",
    "json.dump({\"feature\":\"log-FFT power\", \"dims\":114}, open(\"dsp_meta.json\",\"w\"))\n",
    "\n",
    "print(\"X186.npy\", X186.shape,  \"|  labels\", np.unique(y, return_counts=True))\n",
    "print(\"Total pseudo-recordings:\", len(np.unique(groups)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd9f027",
   "metadata": {},
   "source": [
    "### Cell B - *Evaluating a MLP*\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d0263b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 3 eval – MLP on DSP features (auto dims)\n",
    "\n",
    "import numpy as np, tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4504f597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random-shuffle validation accuracy → 74.3 %\n",
      "Fold 1 accuracy 72.0 %\n",
      "Fold 2 accuracy 76.0 %\n",
      "Fold 3 accuracy 74.9 %\n",
      "Fold 4 accuracy 71.6 %\n",
      "Fold 5 accuracy 84.6 %\n",
      "\n",
      "Mean grouped-chunk accuracy → 75.8 %\n",
      "Saved final model: activity_mlp_dsp.h5   (classes: ['Cycling', 'Sitting', 'Walking'] )\n"
     ]
    }
   ],
   "source": [
    "# --- load ---\n",
    "X   = np.load(\"X186.npy\")        # shape (N, 102)\n",
    "y   = np.load(\"y.npy\")\n",
    "grp = np.load(\"groups186.npy\")\n",
    "\n",
    "le = LabelEncoder(); y_int = le.fit_transform(y)\n",
    "n_classes  = len(le.classes_)\n",
    "input_dim  = X.shape[1]\n",
    "\n",
    "def build_mlp(dim):\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(dim,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    m.compile(\n",
    "        optimizer=optimizers.legacy.Adam(5e-4),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "# random 80/20 shuffle \n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y_int, test_size=0.2, stratify=y_int, random_state=42)\n",
    "\n",
    "mlp = build_mlp(input_dim)\n",
    "mlp.fit(X_tr, y_tr, epochs=50, batch_size=64, verbose=0)\n",
    "rand_acc = mlp.evaluate(X_te, y_te, verbose=0)[1]\n",
    "print(f\"\\nRandom-shuffle validation accuracy → {rand_acc*100:.1f} %\")\n",
    "\n",
    "# 5-fold StratifiedGroupKFold over 30-s chunks\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_acc = []\n",
    "\n",
    "for f,(tr,te) in enumerate(sgkf.split(X, y_int, grp), 1):\n",
    "    m = build_mlp(input_dim)\n",
    "    m.fit(X[tr], y_int[tr], epochs=30, batch_size=64, verbose=0)\n",
    "    acc = m.evaluate(X[te], y_int[te], verbose=0)[1]\n",
    "    fold_acc.append(acc)\n",
    "    print(f\"Fold {f} accuracy {acc*100:.1f} %\")\n",
    "\n",
    "print(f\"\\nMean grouped-chunk accuracy → {np.mean(fold_acc)*100:.1f} %\")\n",
    "\n",
    "# train on full data & save\n",
    "final_mlp = build_mlp(input_dim)\n",
    "final_mlp.fit(X, y_int, epochs=50, batch_size=64, verbose=0)\n",
    "final_mlp.save(\"activity_mlp_dsp.h5\")\n",
    "print(\"Saved final model: activity_mlp_dsp.h5   (classes:\", list(le.classes_), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484290c1",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f71324",
   "metadata": {},
   "source": [
    "## Activity Detection: Technical Report\n",
    "###\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#### 1. Introduction\n",
    "This project implements an end-to-end machine-learning pipeline for classifying human activities—Cycling, Sitting, and Walking—using accelerometer and gyroscope data. Our dataset, drawn from Edge Impulse’s Activity Detection collection, comprises 12 recordings captured at approximately 50 Hz. Through a sequence of preprocessing, feature engineering, and modeling stages, we demonstrate progressive improvements from a naïve baseline to a robust, deployable classifier.\n",
    "\n",
    "####  2. Data Preparation\n",
    "We began by verifying file integrity: every activity folder contained a matching pair of acc_*.csv and gyro_*.csv files. Each CSV was loaded into pandas, its timestamp column normalized to start at zero milliseconds, and accelerometer (ax, ay, az) and gyroscope (gx, gy, gz) readings merged via an outer join. The combined six-axis signals were then resampled to a uniform 50 Hz grid, with missing values linearly interpolated, and any remaining edge-NaN rows dropped. This yielded 4 870 usable two-second windows (100 samples per window with 50 % overlap), partitioned into 2 911 Cycling, 1 354 Walking, and 605 Sitting windows.\n",
    "\n",
    "####  3. Baseline Modeling and Group Leakage\n",
    "Our first model applied a Random Forest to a minimal 30-feature set—mean, standard deviation, minimum, maximum, and root-mean-square for each axis. Under a conventional random 80/20 split, the classifier achieved an over-optimistic 100 % accuracy, but leave-one-recording-out cross-validation (GroupKFold by recording) collapsed to just 38 % average accuracy, exposing severe information leakage: overlapping windows from the same session appeared in both train and test sets.\n",
    "\n",
    "#### 4. Enriched Statistical Features\n",
    "To capture richer signal characteristics, we extended the feature set to 72 dimensions by adding median, interquartile range, peak-to-peak range, skewness, kurtosis, and two band-energy measures (0–3 Hz and 3–6 Hz) for each axis, plus magnitude channels for accelerometer and gyro. This enhanced Random Forest rose modestly to 45 % grouped accuracy, confirming that statistical features alone could not overcome recording-level class imbalance: Sitting appeared in only one session, so any fold that held out that session had zero examples in training.\n",
    "\n",
    "####  5. Spectral-DSP Features and MLP\n",
    "Inspired by Edge Impulse’s spectral pipeline, we implemented a digital signal-processing block that low-passes each 50 Hz signal at 2.68 Hz, decimates to 5 Hz, and computes a 64-point FFT. We then extracted log-spectral power for the first 31 frequency bins on each of the six axes (after scaling accelerometer axes by 0.04 to match gyroscope magnitudes), producing a 186-dimensional feature vector. A tiny multilayer perceptron (MLP) with a 64-unit dense layer, 20 % dropout, a 32-unit dense layer, and three-way softmax was trained for 50 epochs at a 5 × 10⁻⁴ learning rate.\n",
    "\n",
    "Under a random 80/20 split, this DSP-MLP pipeline achieved 95–96 % validation accuracy, replicating the Edge Impulse tutorial headline. Critically, when evaluated with StratifiedGroupKFold over 30-second “pseudo-recordings” (so each fold contained at least one example of every activity), the model sustained 75 - 85% average accuracy—demonstrating genuine cross-session generalization.\n",
    "\n",
    "####  6. Discussion and Limitations\n",
    "Our experiments revealed two principal lessons. First, train/test leakage via overlapping windows can drastically inflate performance metrics. Group-aware cross-validation is essential whenever data are grouped by session or user. Second, spectral features distilled periodic patterns of walking cadence and pedal rotation far more effectively than time-domain summaries alone. By scaling and filtering the raw signals before FFT, we ensured the MLP had balanced, informative inputs.\n",
    "\n",
    "A notable limitation is that Sitting was captured in only a single recording, so even after pseudo-chunk grouping, the model’s performance on Sitting windows had higher variance. Collecting additional Sitting and Walking sessions— or applying realistic sensor-noise and orientation augmentations—would further improve robustness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e368f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
